{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b13a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113b65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2915d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e9b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9726fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e40e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1fe9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5d963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3c08c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf8b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff2ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11c775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0faa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e982805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f7d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb146e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb224ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e6098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ccf6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe9481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b216e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb48ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4843698",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. RNN\n",
    "\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.datasets import imdb\n",
    "from random import randint\n",
    "\n",
    "# Load the IMDB dataset, limiting to the top 5000 most frequent words\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=80)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=80)\n",
    "\n",
    "# Create the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 128))          # Embedding layer for word embeddings\n",
    "model.add(LSTM(128, activation='tanh', recurrent_activation='sigmoid'))  # LSTM layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=3, validation_data=(x_test, y_test), shuffle=True, verbose=1)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Randomly select a review from the test set for evaluation\n",
    "arr_ind = randint(0, len(x_test) - 1)  # Random index\n",
    "index = imdb.get_word_index()           # Word index dictionary\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()])  # Reverse dictionary for decoding\n",
    "decoded = \" \".join([reverse_index.get(i - 3, \"#\") for i in x_test[arr_ind]])  # Decode review text\n",
    "\n",
    "# Determine the sentiment of the review\n",
    "predicted_sentiment = \"Positive\" if predictions[arr_ind] >= 0.5 else \"Negative\"\n",
    "actual_sentiment = \"Positive\" if y_test[arr_ind] == 1 else \"Negative\"\n",
    "\n",
    "# Print results\n",
    "print(\"Sentence:\", decoded)\n",
    "print(\"Review:\", predicted_sentiment)\n",
    "print(\"Predicted Value:\", predictions[arr_ind][0])\n",
    "print(\"Expected Value:\", y_test[arr_ind])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e6b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. autoencoder\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "input_img = keras.Input(shape=(784,))\n",
    "encoded = layers.Dense(128, activation='sigmoid')(input_img)\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=5,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "encoded_imgs = autoencoder.predict(x_test)\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(encoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ADDING SPARISTY\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "encoding_dim = 32\n",
    "\n",
    "input_img = keras.Input(shape=(784,))\n",
    "# Add a Dense layer with a L1 activity regularizer\n",
    "encoded = layers.Dense(encoding_dim, activation='relu',\n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "\n",
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)\n",
    "\n",
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=20,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "\n",
    "# Encode and decode some digits\n",
    "# Note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "# Use Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#DENOISING AUTOENCODER\n",
    "\n",
    "#Denoising\n",
    "\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "\n",
    "(x_train,_),(x_test,_) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')/255.0\n",
    "x_test = x_test.astype('float32')/255.0\n",
    "print(x_train.shape)\n",
    "\n",
    "\n",
    "x_train=x_train.reshape((len(x_train),np.prod(x_train.shape[1:])))\n",
    "x_test=x_test.reshape((len(x_test),np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "\n",
    "#Architechture\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "#add noise to input\n",
    "noisy_input = GaussianNoise(0.5)(input_img)\n",
    "\n",
    "encoded = Dense(128,activation='sigmoid')(input_img)\n",
    "decoded = Dense(784,activation='sigmoid')(encoded)\n",
    "\n",
    "#Build autencoder model\n",
    "\n",
    "autoencoder = Model(input_img,decoded)\n",
    "#compile\n",
    "autoencoder.compile(optimizer= Adam(),loss='binary_crossentropy')\n",
    "#train\n",
    "autoencoder.fit(x_train,x_train,epochs=5,batch_size=256,shuffle=True,validation_data=(x_test,x_test))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#encode and decode some digits\n",
    "encoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "# Generate noisy images\n",
    "noise_factor = 0.5\n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display noisy\n",
    "    ax = plt.subplot(3, n, i + 1 + n)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(3, n, i + 1 + 2*n)\n",
    "    plt.imshow(encoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c38b6b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. optimizers\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#generate synthetic data\n",
    "np.random.seed(42)\n",
    "X=2*np.random.rand(100,1)\n",
    "y=4+3*X+np.random.randn(100,1)\n",
    "\n",
    "#Add bias term (intercept)\n",
    "X_b=np.c_[np.ones((100,1)),X]\n",
    "\n",
    "\n",
    "# TO PERFORM BATCH GRADIENT DESCENT\n",
    "\n",
    "def batch_gradient_descent(X,y,learning_rate=0.1,n_iterations=1000):      #Function to perform batch Gradient Descent\n",
    "  m=len(y)\n",
    "  theta=np.random.randn(2,1)\n",
    "  print(theta)\n",
    "  for iteration in range(n_iterations):\n",
    "    gradients=2/m*X.T.dot(X.dot(theta)-y)\n",
    "    theta=theta-learning_rate*gradients\n",
    "  return theta\n",
    "\n",
    "theta_bgd=batch_gradient_descent(X_b,y);\n",
    "print(\"BGD Theta:\", theta_bgd)\n",
    "\n",
    "\n",
    "\n",
    "#functions to perform stochastic gradient descent\n",
    "def stochastic_gradient_descent(X,y,learning_rate=0.1,n_iterations=50):\n",
    "  m=len(y)\n",
    "  theta=np.random.randn(2,1)\n",
    "  for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "      random_index=np.random.randint(m)\n",
    "      xi=X[random_index:random_index+1]\n",
    "      yi=y[random_index:random_index+1]\n",
    "      gradients=2*xi.T.dot(xi.dot(theta)-yi)\n",
    "      theta=theta-learning _rate*gradients\n",
    "      return theta\n",
    "\n",
    "theta_sgd=stochastic_gradient_descent(X_b,y)\n",
    "print(\"SGD Theta:\",)\n",
    "\n",
    "\n",
    "# to perform Mini-batch Gradient Descent\n",
    "def mini_batch_gradient_descent(X,y,learning_rate=0.1,n_iterations=50,batch_size=20):\n",
    "  m=len(y)\n",
    "  theta=np.random.randn(2,1)\n",
    "  for iteration in range(n_iterations):\n",
    "    shuffled_indices=np.random.permutation(m)\n",
    "    X_shuffled=X[shuffled_indices]\n",
    "    y_shuffled=y[shuffled_indices]\n",
    "    for i in range(0,m,batch_size):\n",
    "      xi=X_shuffled[i:i+batch_size]\n",
    "      yi=y_shuffled[i:i+batch_size]\n",
    "      gradients=2/len(xi)*xi.T.dot(xi.dot(theta)-yi)\n",
    "      theta=theta-learning_rate*gradients\n",
    "      return theta\n",
    "\n",
    "theta_mbgd=mini_batch_gradient_descent(X_b,y)\n",
    "print(\"Mini-batch GD Theta:\",theta_mbgd)\n",
    "\n",
    "\n",
    "#plotting the result\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_gradient_descent(X,y,theta_bgd,theta_sgd,theta_mbgd):\n",
    "  plt.plot(X,y,\"b.\")\n",
    "  X_new = np.array([[0],[2]])\n",
    "  X_new_b = np.c_[np.ones((2,1)),X_new]\n",
    "  print(X_new)\n",
    "  y_predict_bgd = X_new_b.dot(theta_bgd)\n",
    "\n",
    "  y_predict_sgd = X_new_b.dot(theta_sgd)\n",
    "  y_predict_mbgd = X_new_b.dot(theta_mbgd)\n",
    "  plt.plot(X_new,y_predict_bgd,\"r-\",linewidth = 4,label=\"BGD\")\n",
    "  plt.plot(X_new,y_predict_sgd,\"g-\",linewidth = 2,label=\"SGD\")\n",
    "  plt.plot(X_new,y_predict_mbgd,\"y-\",linewidth=2,label=\"MBGD\")\n",
    "  plt.xlabel(\"$x_1$\")\n",
    "  plt.ylabel(\"$y$\")\n",
    "  plt.legend(loc=\"upper left\")\n",
    "  plt.title(\"Gradient DEscent Comparison\")\n",
    "  plt.show()\n",
    "\n",
    "plot_gradient_descent(X,y,theta_bgd,theta_sgd,theta_mbgd)\n",
    "\n",
    "\n",
    "##Momentum based Gradient Descent\n",
    "\n",
    "def gradient_descent_with_momentum(X , y ,theta , learning_rate , gamma , num_iteration):\n",
    "    m=len(y)\n",
    "    velocity = np.zeros_like(theta)\n",
    "\n",
    "    for i in range(num_iteration):\n",
    "        gradient=(1/m) * X.T.dot(X.dot(theta)-y)\n",
    "        velocity = gamma*velocity + learning_rate * gradient\n",
    "        theta = theta - velocity\n",
    "    return theta\n",
    "\n",
    "X = np.array([[1,2],[3,4] , [5,6]])\n",
    "y = np.array([1,2,3])\n",
    "theta = np.zeros(X.shape[1])\n",
    "print(theta)\n",
    "learning_rate = 0.1\n",
    "gamma = 0.9\n",
    "num_iterations=1000\n",
    "\n",
    "theta = gradient_descent_with_momentum(X , y , theta , learning_rate , gamma , num_iterations)\n",
    "print(theta)\n",
    "\n",
    "\n",
    "#for adagard\n",
    "#for Adagrad\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X=2*np.random.rand(100,1)\n",
    "y=4+3*X+np.random.randn(100,1)\n",
    "\n",
    "#Add bias term (x0=1)to each instance\n",
    "X_b=np.c_[np.ones((100,1)),X]\n",
    "\n",
    "#Parameters for Adagrad\n",
    "learning_rate=0.1\n",
    "epsilon=1e-8\n",
    "num_iterations=1000\n",
    "\n",
    "#Initial parameter vector(theta)\n",
    "theta=np.random.randn(2,1)\n",
    "\n",
    "#Adagrad specific parameters\n",
    "gradient_accum=np.zeros((2,1))\n",
    "\n",
    "#Adagrad optimizer\n",
    "for iteration in range(num_iterations):\n",
    "  gradients=2/len(X_b)*X_b.T.dot(X_b.dot(theta)-y)\n",
    "  gradient_accum += gradients**2\n",
    "  adjusted_gradients=gradients/(np.sqrt(gradient_accum)+epsilon)\n",
    "  theta-=learning_rate*adjusted_gradients\n",
    "\n",
    "print(\"Optimized parameters using Adagrad:\",theta)\n",
    "\n",
    "#plot the result\n",
    "plt.scatter(X,y)\n",
    "plt.plot(X,X_b.dot(theta),color='red',linewidth=2,label='Adagrad')\n",
    "plt.xlabel('X')\n",
    "plt.xlabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#For RMS\n",
    "For RMS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "X_b = np.c_[np.ones((100, 1)), X]  # Add bias term\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epsilon = 1e-8\n",
    "beta = 0.9\n",
    "num_iterations = 1000\n",
    "\n",
    "# Initialize parameters\n",
    "theta = np.random.randn(2, 1)\n",
    "s = np.zeros((2, 1))  # Initialize the running average of squared gradients\n",
    "\n",
    "# RMSprop optimization\n",
    "for iteration in range(num_iterations):\n",
    "    gradients = 2 / len(X_b) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    s = beta * s + (1 - beta) * gradients ** 2  # Update the running average of squared gradients\n",
    "    adjusted_gradients = gradients / (np.sqrt(s) + epsilon)\n",
    "    theta -= learning_rate * adjusted_gradients\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        print(f\"Iteration {iteration}: theta = {theta.ravel()}\")\n",
    "\n",
    "print(\"Theta:\", theta)\n",
    "\n",
    "# Plotting the results\n",
    "plt.scatter(X, y, label='Data')\n",
    "plt.plot(X, X_b.dot(theta), color='red', linewidth=2, label='RMSprop')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ab974",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. CNN USING FILTERS\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imshow, imread\n",
    "from skimage.color import rgb2yuv, rgb2hsv, rgb2gray, yuv2rgb, hsv2rgb\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "#Reading Image\n",
    "dog = imread('/content/sample_data/st,small,507x507-pad,600x600,f8f8f8.jpg')\n",
    "plt.figure(num=None, figsize=(8,6), dpi=80)\n",
    "imshow(dog)\n",
    "\n",
    "\n",
    "#Filter MAtrices\n",
    "\n",
    "sharpen = np.array([[0,-1,0],\n",
    "                    [-1,5,-1],\n",
    "                    [0,-1,0]])\n",
    "\n",
    "blur = np.array([[0.11,0.11,0.11],\n",
    "                    [0.11,0.11,0.11],\n",
    "                    [0.11,0.11,0.11]])\n",
    "\n",
    "\n",
    "vertical = np.array([[-1,0,1],\n",
    "                    [-2,0,2],\n",
    "                    [-1,0,1]])\n",
    "\n",
    "\n",
    "gaussian = (1/16.0) * np.array([[1,2,1],\n",
    "                                [2,4,2],\n",
    "                                [1,2,1]])\n",
    "\n",
    "\n",
    "\n",
    "#plotting the filters\n",
    "\n",
    "fig,ax = plt.subplots(1,3, figsize = (17,10))\n",
    "ax[0].imshow(sharpen, cmap='gray')\n",
    "ax[0].set_title(f'Sharpen', fontsize=18)\n",
    "\n",
    "ax[1].imshow(blur, cmap='gray')\n",
    "ax[1].set_title(f'Blur', fontsize=18)\n",
    "\n",
    "ax[2].imshow(vertical, cmap='gray')\n",
    "ax[2].set_title(f'Vertical', fontsize=18)\n",
    "\n",
    "\n",
    "#Grayscaling Image\n",
    "dog_gray = rgb2gray(dog)\n",
    "plt.figure(num=None, figsize=(8,6), dpi=80)\n",
    "imshow(dog_gray)\n",
    "\n",
    "\n",
    "#Function for applying filters\n",
    "def multi_convolver(image, kernel, iterations):\n",
    "  for i in range(iterations):\n",
    "    image = convolve2d(image, kernel, 'same', boundary = 'fill', fillvalue = 0)\n",
    "  return image\n",
    "\n",
    "convolved_image = multi_convolver(dog_gray, sharpen, 1)\n",
    "\n",
    "plt.figure(num=None, figsize=(8,6), dpi=80)\n",
    "imshow(convolved_image);\n",
    "\n",
    "\n",
    "\n",
    "#For colored Image\n",
    "def convolver_rgb(image, kernel, iterations = 1):\n",
    "  convolved_image_r = multi_convolver(image[:,:,0], kernel, iterations)\n",
    "  convolved_image_g = multi_convolver(image[:,:,1], kernel, iterations)\n",
    "  convolved_image_b = multi_convolver(image[:,:,2], kernel, iterations)\n",
    "\n",
    "  reformed_image = np.dstack((np.rint(abs(convolved_image_r)),np.rint(abs(convolved_image_g)),np.rint(abs(convolved_image_b))))/255\n",
    "\n",
    "  fig,ax = plt.subplots(1,3, figsize = (17,10))\n",
    "\n",
    "  ax[0].imshow(abs(convolved_image_r), cmap='Reds')\n",
    "  ax[0].set_title(f'Red', fontsize=15)\n",
    "\n",
    "  ax[1].imshow(abs(convolved_image_g), cmap='Greens')\n",
    "  ax[1].set_title(f'Green', fontsize=18)\n",
    "\n",
    "  ax[2].imshow(abs(convolved_image_b), cmap='Blues')\n",
    "  ax[2].set_title(f'Blue', fontsize=18)\n",
    "\n",
    "  return np.array(reformed_image*255).astype(np.uint8)\n",
    "\n",
    "#Can add different filters (defined above) here\n",
    "convolved_rgb_gauss = convolver_rgb(dog, vertical.T ,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CNN PARAMETER CALCULATION\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, input_shape=(28,28,3),\n",
    "                 kernel_size = (5,5),\n",
    "                 padding='same',\n",
    "                 use_bias=False))\n",
    "model.add(Conv2D(17, (3,3), padding='same', use_bias=False))\n",
    "model.add(Conv2D(13, (3,3), padding='same', use_bias=False))\n",
    "model.add(Conv2D(7, (3,3), padding='same', use_bias=False))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, input_shape=(28,28,3),\n",
    "                 kernel_size = (5,5),\n",
    "                 padding='same',\n",
    "                 use_bias=True))\n",
    "model.add(Conv2D(17, (3,3), padding='same', use_bias=True))\n",
    "model.add(Conv2D(13, (3,3), padding='same', use_bias=True))\n",
    "model.add(Conv2D(7, (3,3), padding='same', use_bias=True))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, input_shape=(28,28,3),\n",
    "                 kernel_size = (5,5),\n",
    "                 use_bias=False))\n",
    "model.add(Conv2D(17, (3,3), use_bias=False))\n",
    "model.add(Conv2D(13, (3,3), use_bias=False))\n",
    "model.add(Conv2D(7, (3,3), use_bias=False))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, input_shape=(28,28,3),\n",
    "                 kernel_size = (5,5),\n",
    "                 padding='valid',\n",
    "                 use_bias=False))\n",
    "model.add(Conv2D(17, (3,3), padding='valid', use_bias=False))\n",
    "model.add(Conv2D(13, (3,3), padding='valid', use_bias=False))\n",
    "model.add(Conv2D(7, (3,3), padding='valid', use_bias=False))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(10, input_shape=(28,28,3),\n",
    "                 kernel_size = (5,5),\n",
    "                 strides = (1,1),\n",
    "                 padding='valid',\n",
    "                 use_bias=False))\n",
    "model.add(Conv2D(20, (5,5), (2,2), padding='valid', use_bias=False))\n",
    "model.add(Conv2D(40, (5,5), (2,2), padding='valid', use_bias=False))\n",
    "\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(10, input_shape=(39,39,3),\n",
    "                 kernel_size = (3,3),\n",
    "                 padding='valid',))\n",
    "model.add(Conv2D(20, (5,5), (2,2), padding='valid' ))\n",
    "model.add(Conv2D(40, (5,5), (2,2),padding='valid'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MAX POLLING\n",
    "import tensorflow as tf\n",
    "x = tf.constant([[1., 2., 3.],\n",
    "                 [4., 5., 6.],\n",
    "                 [7., 8., 9.]])\n",
    "x = tf.reshape(x, [1, 3, 3, 1])\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid')\n",
    "max_pool_2d(x)\n",
    "\n",
    "\n",
    "\n",
    "x = tf.constant([[1., 2., 3., 4.],\n",
    "                 [5., 6., 7., 8.],\n",
    "                 [9., 10., 11., 12.]])\n",
    "x = tf.reshape(x, [1, 3, 4, 1])\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n",
    "   strides=(2, 2), padding='valid')\n",
    "max_pool_2d(x)\n",
    "\n",
    "\n",
    "x = tf.constant([[1., 2., 3., 4.],\n",
    "                 [5., 6., 7., 8.],\n",
    "                 [9., 10., 11., 12.]])\n",
    "x = tf.reshape(x, [1, 3, 4, 1])\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n",
    "   strides=(2, 2), padding='same')\n",
    "max_pool_2d(x)\n",
    "\n",
    "\n",
    "x = tf.constant([[1., 2., 3., 4.],\n",
    "                 [5., 6., 7., 8.],\n",
    "                 [9., 10., 11., 12.]])\n",
    "x = tf.reshape(x, [1, 3, 4, 1])\n",
    "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n",
    "   strides=(1, 1), padding='same')\n",
    "max_pool_2d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c36e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. REGULARIZATION\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_circles,make_moons\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(800)\n",
    "X,y=make_moons(n_samples=100, noise=0.2, random_state=1)\n",
    "zero_one_colourmap=ListedColormap(('red','green'))\n",
    "rcParams['figure.figsize']=14,7\n",
    "plt.scatter(X[:,0],X[:,1],\n",
    "c=y, s=100,\n",
    "cmap=zero_one_colourmap)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=1000, verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "reg_model = Sequential()\n",
    "reg_model.add(Dense(500, input_dim=2, activation='relu', kernel_regularizer='l2'))\n",
    "reg_model.add(Dense(1, activation='sigmoid', kernel_regularizer='l2'))\n",
    "reg_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "reg_history = reg_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, verbose=0)\n",
    "\n",
    "plt.plot(reg_history.history['loss'], label='train')\n",
    "plt.plot(reg_history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b96fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. AUGUMENTION\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255  # Reshape and normalize\n",
    "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Data Augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(28, 28, 1)),  # Specify input shape here\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# Create a simple CNN model\n",
    "def create_cnn_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(data_augmentation)  # Include data augmentation in the model\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))  # Output layer\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_cnn_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model with data augmentation\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='test accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='test loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd963dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. DROP OUT\n",
    "\n",
    "# DropOut with CNN on Mnist dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Build CNN model with Dropout\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Dropout layer with 50% rate\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Plot training & validation accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. CNN ON MINIST DATASET USING FILTERS\n",
    "\n",
    "# Filter on Mnist using CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255  # Reshape and normalize\n",
    "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Define the CNN model\n",
    "def create_cnn_model():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Convolutional layer with 3x3 filters (Sharpening Filter)\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "\n",
    "    # Convolutional layer with 5x5 filters (Blur Filter)\n",
    "    model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "\n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Another convolutional layer (Edge Detection Filter)\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))  # Dropout for regularization\n",
    "    model.add(layers.Dense(10, activation='softmax'))  # Output layer\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_cnn_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='test accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='test loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
